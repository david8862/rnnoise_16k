#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''Tool to evaluate VAD metrics, generated by DeepSeek :D
Reference from:
https://www.deepseek.com
https://blog.csdn.net/matrix_space/article/details/50384518
https://blog.csdn.net/zjn295771349/article/details/84961596
'''
import os, sys, argparse
import glob
from tqdm import tqdm
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt


def find_voice_segments(labels, min_silence_duration=30):
    """
    从VAD标签序列中找出连续的语音段

    Args:
        labels: VAD标签序列 (0=静音, 1=语音)
        min_silence_duration: 被认为是一个新语音段的最小静音帧数

    Returns:
        List of (start_frame, end_frame) tuples for each voice segment
    """
    segments = []
    in_segment = False
    start_frame = 0

    for i, label in enumerate(labels):
        if label == 1 and not in_segment:
            # 语音段开始
            in_segment = True
            start_frame = i
        elif label == 0 and in_segment:
            # 检查是否真的结束（避免短暂的静音打断语音段）
            # 查看接下来的min_silence_duration帧是否都是静音
            lookahead_end = min(i + min_silence_duration, len(labels))
            if all(labels[i:lookahead_end] == 0):
                in_segment = False
                segments.append((start_frame, i-1))

    # 处理最后一个语音段
    if in_segment:
        segments.append((start_frame, len(labels)-1))

    return segments


def calculate_overlap(seg1, seg2):
    """
    计算两个语音段的重叠比例

    Args:
        seg1, seg2: (start_frame, end_frame) tuples

    Returns:
        重叠比例 (0-1)
    """
    start1, end1 = seg1
    start2, end2 = seg2

    # 计算重叠部分
    overlap_start = max(start1, start2)
    overlap_end = min(end1, end2)

    if overlap_start > overlap_end:
        return 0.0

    overlap_length = overlap_end - overlap_start + 1
    seg1_length = end1 - start1 + 1

    return overlap_length / seg1_length



def calculate_vad_event_metrics(reference_labels, predicted_labels, min_silence_duration=30, overlap_threshold=0.3):
    """
    计算VAD的基于事件的性能指标

    Args:
        reference_labels: 参考VAD标签 (0=静音, 1=语音)
        predicted_labels: 预测VAD标签 (0=静音, 1=语音)
        min_silence_duration: 被认为是一个新语音段的最小静音帧数
        overlap_threshold: 判断为"命中"的最小重叠比例阈值

    Returns:
        包含各种指标的字典
    """
    # 找出语音段
    ref_segments = find_voice_segments(reference_labels, min_silence_duration)
    pred_segments = find_voice_segments(predicted_labels, min_silence_duration)

    # 初始化统计
    hits = 0
    deletions = 0
    insertions = 0

    # 标记哪些段已经被匹配
    ref_matched = [False] * len(ref_segments)
    pred_matched = [False] * len(pred_segments)

    # 第一遍：寻找命中（重叠超过阈值）
    for i, ref_seg in enumerate(ref_segments):
        for j, pred_seg in enumerate(pred_segments):
            if not pred_matched[j]:
                overlap = calculate_overlap(ref_seg, pred_seg)
                if overlap >= overlap_threshold:
                    hits += 1
                    ref_matched[i] = True
                    pred_matched[j] = True
                    break

    # 统计删除错误（未匹配的参考语音段）
    deletions = len(ref_segments) - hits

    # 统计插入错误（未匹配的预测语音段）
    insertions = len(pred_segments) - hits

    # 计算错误率
    total_reference_segments = len(ref_segments)
    if total_reference_segments == 0:
        deletion_rate = 0.0
        insertion_rate = 0.0
    else:
        deletion_rate = deletions / total_reference_segments
        insertion_rate = insertions / total_reference_segments

    # 计算基于时间的指标（可选）
    total_reference_frames = np.sum(reference_labels)
    total_predicted_frames = np.sum(predicted_labels)

    # 计算帧级的准确率指标作为补充
    true_positives = np.sum((reference_labels == 1) & (predicted_labels == 1))
    false_positives = np.sum((reference_labels == 0) & (predicted_labels == 1))
    false_negatives = np.sum((reference_labels == 1) & (predicted_labels == 0))

    accuracy = np.mean(predicted_labels == reference_labels)
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'hits': hits,
        'deletions': deletions,
        'insertions': insertions,
        'deletion_rate': deletion_rate,
        'insertion_rate': insertion_rate,
        'total_reference_segments': total_reference_segments,
        'total_predicted_segments': len(pred_segments),
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'reference_segments': ref_segments,
        'predicted_segments': pred_segments
    }


def visualize_result(annotation_array, result_array, audio_duration, height=480, width=640, barrier_height=20):
    # create visualize annotation voice segment data
    expanded_annotation_array = np.expand_dims(annotation_array, axis=0).astype(np.uint8)
    annotation_image = Image.fromarray(expanded_annotation_array)
    annotation_image = annotation_image.resize((width, 1), Image.BICUBIC)
    annotation_data = np.asarray(annotation_image).astype(np.uint8)
    annotation_data = np.tile(annotation_data, ((height-barrier_height)//2, 1))

    # create visualize predicted voice segment data
    expanded_result_array = np.expand_dims(result_array, axis=0).astype(np.uint8)
    result_image = Image.fromarray(expanded_result_array)
    result_image = result_image.resize((width, 1), Image.BICUBIC)
    result_data = np.asarray(result_image).astype(np.uint8)
    result_data = np.tile(result_data, ((height-barrier_height)//2, 1))

    # create barrier to separate annotation data & predict data
    barrier_array = np.zeros((barrier_height, width), dtype=np.uint8)

    # conate predict data, barrier & annotation data
    visualize_array = np.concatenate((result_data, barrier_array), axis=0)
    visualize_array = np.concatenate((visualize_array, annotation_data), axis=0)

    # set chart title & label
    plt.title('VAD annotation & prediction', fontsize='large')
    plt.xlabel('Time(seconds)', fontsize='large')
    #plt.ylabel('Annotation (down) & Prediction (up)', fontsize='large')

    # use time as x-ticks, label as y-ticks
    xticks_labels = range(0, int(audio_duration), int(audio_duration)//10)
    plt.xticks(np.linspace(0, width, num=len(xticks_labels), endpoint=True), xticks_labels)
    yticks_labels = ['', 'Prediction', '', 'Annotation', '']
    plt.yticks(np.linspace(0, height, num=len(yticks_labels), endpoint=True), yticks_labels)

    # show chart
    plt.imshow(visualize_array)
    plt.grid(axis='x')
    plt.show()



def vad_eval(annotation_txt_path, result_txt_path, min_silence_duration, overlap_threshold, visualize):
    # get annotation txt file list or single annotation txt file
    if os.path.isfile(annotation_txt_path):
        annotation_txt_list = [annotation_txt_path]
    else:
        annotation_txt_list = glob.glob(os.path.join(annotation_txt_path, '*.txt'))
        annotation_txt_list.sort()

    # get result txt file list or single result txt file
    if os.path.isfile(result_txt_path):
        result_txt_list = [result_txt_path]
    else:
        result_txt_list = glob.glob(os.path.join(result_txt_path, '*.txt'))
        result_txt_list.sort()

    # file number should match
    assert (len(annotation_txt_list) == len(result_txt_list)), 'annotation file number mismatch with result file'

    total_annotation_vad = np.empty(0, dtype=int)
    total_result_vad = np.empty(0, dtype=int)
    #total_sample_num = 0

    # check every annotation txt file
    pbar = tqdm(total=len(annotation_txt_list), desc='VAD evaluation')
    for annotation_txt_filename in annotation_txt_list:
        # search for corresponding result txt file
        if len(result_txt_list) == 1:
            result_txt_filename = result_txt_list[0]
        else:
            annotation_txt_basename = os.path.basename(annotation_txt_filename)
            result_txt_filenames = [item for item in result_txt_list if annotation_txt_basename in item]
            # check if there is dumplate result txt file
            assert (len(result_txt_filenames) == 1), 'dumplate result txt file:{}'.format(result_txt_filenames)
            result_txt_filename = result_txt_filenames[0]

        # voice timestamps txt file would be like:
        # cat voice_timestamp.txt
        # 16000               // sample rate
        # 8525.995            // total duration time (in seconds)
        # 197.949,199.200     // voice start/stop time (in seconds)
        # 200.159,202.829
        # ...
        annotation_txt_file = open(annotation_txt_filename, "r", encoding='utf-8')
        annotation_lines = annotation_txt_file.readlines()
        annotation_sample_rate = int(annotation_lines[0].strip())
        annotation_audio_duration = float(annotation_lines[1].strip())

        # create annotation vad array for the whole audio
        annotation_sample_num = int(annotation_audio_duration * annotation_sample_rate)
        annotation_vad = np.zeros(annotation_sample_num, dtype=int)

        # assign annotation voice samples to 1 according to voice start/stop time
        for annotation_segment in annotation_lines[2:]:
            start_time, stop_time = annotation_segment.split(',')
            start_sample = int(float(start_time) * annotation_sample_rate)
            stop_sample = int(float(stop_time) * annotation_sample_rate)
            annotation_vad[start_sample:(stop_sample+1)] = 1

        # parse result txt file with the same format
        result_txt_file = open(result_txt_filename, "r", encoding='utf-8')
        result_lines = result_txt_file.readlines()
        result_sample_rate = int(result_lines[0].strip())
        result_audio_duration = float(result_lines[1].strip())

        # check if sample rate & audio duration match annotation
        assert (result_sample_rate == annotation_sample_rate), 'sample rate mismatch between annotation & result for {}'.format(annotation_txt_filename)
        assert (abs(result_audio_duration - annotation_audio_duration) <= 0.01), 'audio duration mismatch between annotation & result for {}'.format(annotation_txt_filename)
        # ignore minor duration mismatch (maybe due to rounding error), for further array alignment
        result_audio_duration = annotation_audio_duration

        # create result vad array for the whole audio
        result_sample_num = int(result_audio_duration * result_sample_rate)
        result_vad = np.zeros(result_sample_num, dtype=int)

        # assign result voice samples to 1 according to voice start/stop time
        for result_segment in result_lines[2:]:
            start_time, stop_time = result_segment.split(',')
            start_sample = int(float(start_time) * result_sample_rate)
            stop_sample = int(float(stop_time) * result_sample_rate)
            result_vad[start_sample:(stop_sample+1)] = 1

        # merge single audio vad result into total result
        total_annotation_vad = np.concatenate((total_annotation_vad, annotation_vad), axis=0)
        total_result_vad = np.concatenate((total_result_vad, result_vad), axis=0)
        #total_sample_num += annotation_sample_num
        pbar.update(1)
    pbar.close()

    # 计算指标
    metrics = calculate_vad_event_metrics(total_annotation_vad, total_result_vad, min_silence_duration, overlap_threshold)
    match_rate = float(metrics['hits']) / metrics['total_reference_segments']

    print("VAD metric report:")
    print("=" * 50)
    print(f"hit number: {metrics['hits']}")
    print(f"match rate: {match_rate}")
    print(f"deletion error number: {metrics['deletions']}")
    print(f"insertion error number: {metrics['insertions']}")
    print(f"deletion error rate: {metrics['deletion_rate']:.3f} ({metrics['deletion_rate']*100:.1f}%)")
    print(f"insertion error rate: {metrics['insertion_rate']:.3f} ({metrics['insertion_rate']*100:.1f}%)")
    print(f"reference speech segment number: {metrics['total_reference_segments']}")
    print(f"predicted speech segment number: {metrics['total_predicted_segments']}")
    print("-" * 30)
    print(f"accuracy: {metrics['accuracy']:.3f}")
    print(f"precision: {metrics['precision']:.3f}")
    print(f"recall: {metrics['recall']:.3f}")
    print(f"F1 score: {metrics['f1_score']:.3f}")

    if visualize:
        visualize_result(total_annotation_vad, total_result_vad, annotation_audio_duration)


def main():
    parser = argparse.ArgumentParser(description='tool to evaluate VAD metrics')
    parser.add_argument('--annotation_txt_path', type=str, required=True,
                        help='file or directory for voice timestamp annotation txt file')
    parser.add_argument('--result_txt_path', type=str, required=True,
                        help='file or directory for voice timestamp detect result txt file')
    parser.add_argument('--min_silence_duration', type=int, required=False, default=30,
                        help='minimum silence sample number to start a new voice segment. default=%(default)s')
    parser.add_argument('--overlap_threshold', type=float, required=False, default=0.3,
                        help='threshold of overlap rate for result segment to match an annotation segment. default=%(default)s')
    parser.add_argument('--visualize', default=False, action="store_true",
                        help='Whether to visualize VAD result')

    args = parser.parse_args()

    vad_eval(args.annotation_txt_path, args.result_txt_path, args.min_silence_duration, args.overlap_threshold, args.visualize)


if __name__ == "__main__":
    main()
